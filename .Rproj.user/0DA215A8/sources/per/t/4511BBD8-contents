---
title: "Brehm_Gabriel_HW1"
author: "Gabriel Brehm"
date: "2023-02-09"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ggplot2)
library(rdist)
library(ggm)

```

# Problem 1
## a)
```{=Latex}
Show: $\mathbf{E} \left[ \frac{1}{d} \; || \mathbf{X} - \mathbf{X'} ||_2^2 \right] = \frac{1}{6}$
\vspace{2 mm}
 

\vspace{5 mm}

Consider: 
\vspace{2 mm}

$|| \mathbf{X} - \mathbf{X} ||_2^2 = || \mathbf{X} ||_2^2 + || \mathbf{X'} ||_2^2 - 2\langle \mathbf{X}, \mathbf{X'} \rangle$
\vspace{2 mm}

$= \sum{\mathbf{x}_j^2} + \sum{\mathbf{x'}_j^2 - 2\sum{x_j x'_j}}$

\vspace{2 mm}

$\mathbf{E} \left[ x^2 \right] = \frac{1}{3}$
\vspace{5 mm}

Then:
\vspace{2 mm}

$\mathbf{E} \left[ \frac{1}{d} \; || \mathbf{X} - \mathbf{X'} ||_2^2 \right] = \frac{1}{d}\mathbf{E} \left[ \sum{\mathbf{x}_j^2} + \sum\mathbf{x'}_j^2 - 2\sum{x_j x'_j} \right] $ by the above identities (and linearity of expectation to pull the constant out).
\vspace{2 mm}

Using linearity of expectation, we find:
\vspace{2 mm}

$\frac{1}{d}\mathbf{E} \left[ \sum{\mathbf{x}_j^2} + \sum\mathbf{x'}_j^2 - 2\sum{x_j x'_j} \right] = \frac{1}{d}\mathbf{E}\left[\sum{\mathbf{x}_j^2}\right] + \frac{1}{d}\mathbf{E}\left[\sum\mathbf{x'}_j^2\right] - \frac{2}{d}\mathbf{E}\left[\sum{x_j x'_j}\right]$
\vspace{2 mm}

Each of the $x_j$'s and $x'_j$ come from the same distribution, meaning that a sum of d of them is equivalent to multiplying them by d: $\frac{1}{d}\mathbf{E}\left[\sum\mathbf{x}_j^2\right] = \frac{1}{d}\mathbf{E}\left[d\mathbf{x}_j^2\right] = \frac{d}{d}\mathbf{E}\left[\mathbf{x}_j^2\right]$ 
\vspace{2 mm}

Clearly the d's cancel as $d > 0$. Using the independence of the given RVs and linearity of expectation (our best friend), we can see that:
\vspace{2 mm}

$\frac{2}{d}\mathbf{E}\left[\sum{x_j x'_j}\right] = \frac{2}{d}\sum{\mathbf{E}\left[x_j x'_j\right]} = \frac{2}{d}\sum\left[\mathbf{E}\left[x_j\right] \mathbf{E}\left[{x'_j}\right]\right] = \frac{d}{d}\mathbf{E}\left[x_j\right] \mathbf{E}\left[{x'_j}\right]$
\vspace{2 mm}

Once again the d's cancel. The expectation for U(0, 1) is 1/2, which will be used without proof. Using the given $\mathbf{E} \left[ x^2 \right] = \frac{1}{3}$, we finally have:
\vspace{2 mm}

$\frac{1}{3} + \frac{1}{3} - 2\cdot \frac{1}{2} \cdot \frac{1}{2} = \frac{2}{3} - \frac{1}{2} = \frac{1}{6}$
\vspace{2 mm}

Thus proving the desired property.

```

## b)
```{=Latex}
Assume $\mathbf{Z} = \frac{1}{d}|| \mathbf{X} - \mathbf{X'} ||_2^2$. Then:
\vspace{2 mm}

$\mathbf{P} \left( \left| \frac{1}{d} \sum_{i=1}^{d}{Z_i} - \mathbf{E} \left[Z \right] \right| \geq \frac{t}{\sqrt{d}}\right) \leq 2exp \left(-2t^2 \right)$
\vspace{2 mm}

$= \mathbf{P} \left( \frac{1}{d} \sum_{i=1}^{d}{Z_i} - \mathbf{E} \left[Z \right] \geq \pm \frac{t}{\sqrt{d}}\right) \leq 2exp \left(-2t^2 \right)$
\vspace{2 mm}

$= \mathbf{P} \left( \frac{1}{d} \sum_{i=1}^{d}{Z_i} \geq \mathbf{E} \left[Z \right] \pm \frac{t}{\sqrt{d}}\right) \leq 2exp \left(-2t^2 \right)$
\vspace{2 mm}

Here, choosing $t = \sqrt{log(d)}$ will give desirable results. Also remember that $\mathbf{E} \left[Z \right] = \frac{1}{6}$:
\vspace{2 mm}

$= \mathbf{P} \left( \frac{1}{d} \sum_{i=1}^{d}{Z_i} \geq \frac{1}{6} \pm \sqrt{\frac{log(d)}{d}}\right) \leq 2exp \left(-2\sqrt{log(d)}^2 \right)$
\vspace{2 mm}

$= \mathbf{P} \left( \frac{1}{d} \sum_{i=1}^{d}{Z_i} \geq \frac{1}{6} \pm \sqrt{\frac{log(d)}{d}}\right) \leq 2exp \left(-2\log(d) \right)$
\vspace{2 mm}

$= \mathbf{P} \left( \frac{1}{d} \sum_{i=1}^{d}{Z_i} \geq \frac{1}{6} \pm \sqrt{\frac{log(d)}{d}}\right) \leq 2d^{-2} = \frac{2}{d^2}$
\vspace{2 mm}

Then:
\vspace{2 mm}

$\mathbf{P} \left( \frac{1}{d} \sum_{i=1}^{d}{Z_i} \geq \frac{1}{6} \pm \sqrt{\frac{log(d)}{d}}\right) = 1 - \mathbf{P} \left( \frac{1}{d} \sum_{i=1}^{d}{Z_i} \leq \frac{1}{6} \pm \sqrt{\frac{log(d)}{d}}\right)$
\vspace{2 mm}

Then:
\vspace{2 mm}

$\mathbf{P} \left( \frac{1}{d} \sum_{i=1}^{d}{Z_i} \leq \frac{1}{6} \pm \sqrt{\frac{log(d)}{d}}\right) \geq 1 - \frac{2}{d^2}$
\vspace{2 mm}

This shows that the average point (given by $\frac{1}{d} \sum_{i=1}^{d}{Z_i}$) falls within the interval $\frac{1}{6} \pm \sqrt{\frac{log(d)}{d}}$ with probability of at least $1 - \frac{2}{d^2}$.
```

## c)

```{r data_pull}
### Leukaemia data set

leuk <- read.csv("leukemia_small.csv")
n <- ncol(leuk)
isALL <- numeric(ncol(leuk))
isALL[grep("ALL", colnames(leuk))] <- 1
leuk <- leuk[,c(which(isALL == 1), which(isALL == 0))]
```

### i)

```{r prechecks}
### Print the largest row mean and compare to the smallest data values (in terms of distance from 0) to verify that the means are small enough for our tastes.
print(max(rowMeans(leuk)))
print(min(abs(leuk)))

apply(leuk, 2, sd)

```

The largest row mean is extremely small relative to the value closest to zero in the entire data set, indicating that the row means are functionally zero.

### ii) Pairwise Distances 
```{r calculate_distances}
# calculate distances
distances <- pdist(leuk, "euclidean")

# scale the distances
distances.scaled <- distances/sqrt(ncol(leuk) - 1)



```

### iii) Boxplot
```{r produce_boxplot}
boxplot(as.vector(distances.scaled))

```
The "tail" on the boxplot between 0 and the bottom of the box indicates that the data is probably not entirely random, and that there is likely some signal in the noise.

# Problem 2
## a)
```{=Latex}
For: $Cov(Y|X)$ = $\Sigma_Y - \Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}$
Show: $tr(Cov(Y|X))$ $\leq$ tr($\Sigma_Y$)
\vspace{5 mm}

Idea: Show that the thing being subtracted is symmetric positive definite; this implies when it's subtracted from $\Sigma_Y$, the resulting matrix will have smaller eigenvalues and therefore a smaller trace than $\Sigma_Y$. All covariance matrices are positive definite, so all that is left to show is that it is symmetric:
\vspace{2 mm}

$\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY} \stackrel{?}{=} \left(\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}\right)^T$
\vspace{1 mm}

$\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY} \stackrel{?}{=} \left(\left(\Sigma_{YX}\Sigma_{XX}^{-1}\right)\Sigma_{XY}\right)^T$
\vspace{1 mm}

$\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY} \stackrel{?}{=} \Sigma_{XY}^T\left(\Sigma_{YX}\Sigma_{XX}^{-1}\right)^T$
\vspace{1 mm}

$\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY} \stackrel{?}{=} \Sigma_{XY}^T\Sigma_{XX}^{-1T}\Sigma_{YX}^T$
\vspace{1 mm}

$\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY} = \Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}$
\vspace{2 mm}

Therefore by the above argument, $tr(Cov(Y|X))$ $\leq$ tr($\Sigma_Y$)

```

## b)

```{r math_scores}
data(marks)

Sigma <- cor(marks)
Sigma.inv <- solve(Sigma)

cor.partial <- -cov2cor(Sigma.inv)
diag(cor.partial) <- 1

cor.partial 
```

In short, we can use Dawid's notation to say:
```{=Latex}
$mechanics, vectors \perp analysis, statistics | algebra$
```

# Problem 3

```{=Latex}
We will take as given that a normal distribution squared is equal to a chi-squared distribution with degrees of freedom relative to the variance of the normal distribution in question:
\vspace{2 mm}

For $\mathbf{X} \sim N_d(\mu, \Sigma)$, $\mathbf{X^2} \sim \chi_{p}^2$
\vspace{2 mm}

Then to show that the Mahalanobis distance is in the chi-squared distribution, we merely need to show it as the square of a normal RV. Take:
\vspace{2 mm}

$\mathbf{Q} = \Sigma^{-1/2}(\mathbf{X} - \mu)$
\vspace{2 mm}

A transformation of the RV $\mathbf{X}$ with $\Sigma^{-1/2}$ as the square root of the inverse covariance, and a constant. Then we see:
\vspace{2 mm}

$(\mathbf{X} - \mu)^T\Sigma^{-1}(\mathbf{X} - \mu) = Q^T Q$
\vspace{2 mm}

For each element in the $\vec{RV}$ resulting from $Q^T Q$, a normal RV is multiplied with an equivalent normal RV, resulting in the square of a normal RV which is a chi-squared distribution as stated above:
\vspace{2 mm}

$Q^T Q = \sum_{i=1}^{d}X_{i}^2 = \sum_{i=1}^{d}\chi^2 = \chi_{d}^2$
\vspace{2 mm}

I am very certain that $\mathbf{Q} \sim N_d(0, 1)$ as but I cannot figure out how to prove the part about the variance. This is unfortunate because that is the final component needed to show that the above expression is a chi-squared distribution with degrees of freedom d.
```